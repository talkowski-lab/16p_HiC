---
title: "Loop Analysis for 16p"
author: "Siddharth Reed"
date: "`r Sys.Date()`"
---
<style type="text/css">
.main-container {
  max-width: 1800px !important;
  margin-left: auto;
  margin-right: auto;
}
</style>

# Dependencies + Load data

```{r knitr}
library(knitr)
knitr::opts_chunk$set(
    results=FALSE,
    message=FALSE,
    warning=FALSE,
    dev=c('png', 'pdf', 'svg'),
    dpi=300
)
```
Set up file locations + load dependences
```{r dependencies}
library(here)
here::i_am('notebooks/Loops.Rmd')
BASE_DIR <- here()
source(file.path(BASE_DIR, 'scripts/locations.R'))
source(file.path(SCRIPT_DIR, 'utils.data.R'))
source(file.path(BASE_DIR, 'scripts/constants.R'))
source(file.path(SCRIPT_DIR, 'utils.plot.R'))
source(file.path(SCRIPT_DIR, 'utils.annotations.R'))
source(file.path(SCRIPT_DIR, 'loops/utils.loops.R'))
library(tidyverse)
library(magrittr)
```
Using the default params we only keep loops < 2Mb in size, which is enough to capture most CTCF-associated loops in human cells. The loop calling params are explained in the [cooltools docs](https://cooltools.readthedocs.io/en/latest/notebooks/dots.html#Dot-calling-with-default-parameters):

- A set of convolution kernels are recommended based on the resolution of clr, if user-defined convolution kernels are not provided (i.e. kernels=None).
- The requested portion of the heatmap (defined by view_df and max_loci_separation) is split into smaller tiles of size tile_size. This ensures the entire heatmap is not loaded into memory at once, and computationally intensive steps can be done in parallel using nproc workers. tile_size and nproc do not affect the outcome of the procedure.
- Tiles of the heatmap are convolved with the provided kernels to calculate localy adjusted expected for each pixel. This is in turn used to calculate p-values, assuming a Poisson distribution of pixel counts.
- Pixels are assigned to geometrically-spaced “lambda-bins” of locally-adjusted expected for statistical testing. Within each lambda-bin, signficantly enriched pixels are “caled” using BH-FDR multiple hypothesis testing procedure, and thresholds of significance are calculated for each lambda-bin and each kernel-type (controlled by lambda_bin_fdr).
- Significantly-enriched pixels are extracted, based on the thresholds in each lambda bin. Note the cooltools implementation of this step involves a second pass with the same convolution kernels to re-score pixels, as this is less costly than storing all such scores in memory.
- Additional clustering and empirical filtering is optionally performed (depending on clustering_radius and cluster_filtering).

Load all cooltools loop annotations, only keep 1 kernel (donut)
```{r load_loops}
# this is applied to the donut kernel, but all loops are significant with
loops.df <- 
    check_cached_results(
        results_file=ALL_COOLTOOLS_LOOPS_RESULTS_FILE,
        # force_redo=TRUE,
        results_fnc=load_all_cooltools_dots
    ) %>%
    post_process_cooltools_dots_results() %>% 
    filter(kernel == 'donut') %>% 
    standardize_data_cols()
```

# Number of Loops {.tabset}

```{r n_loops}
n.loops.per.chr.df <- 
    loops.df %>%
    # cumulative number of loops at different threshold
    mutate(
        'sig.lvl.qval < 0.000001'=log10.qval > -log10(0.000001),
        'sig.lvl.qval < 0.001'=log10.qval > -log10(0.001),
        'sig.lvl.qval < 0.1'=log10.qval > -log10(0.1),
        'sig.lvl.N.S.'=log10.qval <= -log10(0.1)
    ) %>% 
    select(
        # type, isMerged, 
        weight, resolution, 
        SampleID, 
        chr,
        kernel,
        starts_with('sig.lvl.')
    ) %>% 
    pivot_longer(
        starts_with('sig.lvl.'),
        names_to='sig.lvl',
        names_prefix='sig.lvl.',
        values_to='meet.sig.lvl'
    ) %>% 
    filter(meet.sig.lvl) %>% 
    mutate(
        sig.lvl=
            factor(
                sig.lvl,
                levels=
                    c(
                        'qval < 0.000001',
                        'qval < 0.001',
                        'qval < 0.1',
                        'N.S.'
                    )
            )
    ) %>% 
    count(
        # type, isMerged, 
        weight, resolution, 
        SampleID, 
        chr,
        kernel,
        sig.lvl,
        name='n.loops'
    )
```

## Genome-Wide

```{r plot_n_loops_heatmap_gw, results='asis', fig.dim=c(10,6)}
n.loops.per.chr.df %>% 
    group_by(across(-c(chr, n.loops))) %>% 
    summarize(n.loops=sum(n.loops)) %>%
    ungroup() %>%
    add_column(chr='Genome-Wide') %>% 
    separate_wider_delim(
        SampleID,
        delim='.',
        names=c(NA, 'Celltype', 'Genotype'),
        cols_remove=FALSE
    ) %>%
    # filter(kernel == 'donut') %>% 
    # filter(resolution == '25Kb') %>% 
    filter(sig.lvl %in% c('qval < 0.1', 'N.S.')) %>% 
    make_nested_plot_tabs(
        # group.cols=c('type', 'isMerged', 'weight', 'resolution'),
        group.cols=c('weight', 'resolution'),
        plot.fnc=plot_barplot,
        max.header.lvl=3,
        x.var='SampleID',
        y.var='n.loops',
        fill.var='SampleID', 
        facet.row='sig.lvl',
        facet.col='Celltype',
        legend.position='right',
        scales='free_x',
        # axis.text.x=element_text(angle=45, hjust=1),
        # legend.text=element_text(angle=35, hjust=1),
        axis.title.x=element_blank(),
        axis.title.y=element_blank()
    )
```

## Per Chr

```{r plot_n_loops_heatmap_chr, results='asis', fig.dim=c(10,6)}
n.loops.per.chr.df %>% 
    separate_wider_delim(
        SampleID,
        delim='.',
        names=c(NA, 'Celltype', 'Genotype'),
        cols_remove=FALSE
    ) %>%
    # filter(kernel == 'donut') %>% 
    # filter(resolution == '25Kb') %>% 
    filter(sig.lvl %in% c('qval < 0.1', 'N.S.')) %>% 
    make_nested_plot_tabs(
        # group.cols=c('type', 'isMerged', 'weight', 'resolution'),
        group.cols=c('weight', 'resolution'),
        plot.fnc=plot_barplot,
        max.header.lvl=3,
        x.var='chr',
        x.scale.mode='discrete',
        y.var='n.loops',
        fill.var='SampleID', 
        facet.col='sig.lvl',
        facet.row='Celltype',
        legend.position='right',
        axis.text.x=element_text(angle=45, hjust=1),
        axis.title.x=element_blank(),
        axis.title.y=element_blank()
    )
```

# Size of Loops {.tabset}

Plot size distribution of loops
```{r plot_loop_size_violin, results='asis', fig.dim=c(10,6)}
loops.df %>% 
    make_nested_plot_tabs(
        # group.cols=c('type', 'isMerged', 'weight', 'resolution'),
        group.cols=c('weight', 'resolution'),
        max.header.lvl=2,
        plot.fnc=plot_boxplot,
        # quantile.linetype=TRUE,
        x.var='chr',
        # x.scale.mode='discrete',
        y.var='length',
        y.scale.mode='mb',
        fill.var='SampleID', 
        facet.row='kernel',
        legend.position='right',
        outlier.size=0.25,
        axis.text.x=element_text(angle=45, hjust=1),
        # legend.text=element_text(angle=35, hjust=1),
        axis.title.x=element_blank(),
        axis.title.y=element_blank()
    )
```

# Distribution Plots {.tabset}

Make violin plot with q-values
```{r box_plot, results='asis', fig.dim=c(10,6)}
loops.df %>% 
    pivot_longer(
        c('enrichment', 'log10.qval'),
        names_to='metric', 
        values_to='value'
    ) %>% 
    make_nested_plot_tabs(
        # group.cols=c('type', 'isMerged', 'weight', 'resolution'),
        group.cols=c('weight', 'resolution'),
        max.header.lvl=2,
        plot.fnc=plot_boxplot,
        x.var='chr',
        x.scale.mode='discrete',
        y.var='value',
        fill.var='SampleID',
        # plot_pts=TRUE,
        # facet.col='kernel',
        facet.row='metric',
        scales='free_y',
        outlier.size=0.25,
        strip.text.x=element_text(size=5),
        axis.text.x=element_text(angle=45, hjust=1)
    )
```

# Manhattan Plots {.tabset}

Make manhattan plot with q-values
```{r manhattan_plot, results='asis', fig.dim=c(10,6), eval=FALSE}
loops.df %>% 
    make_nested_plot_tabs(
        # group.cols=c('type', 'isMerged', 'weight', 'resolution'),
        group.cols=c('weight', 'resolution'),
        max.header.lvl=2,
        plot.fnc=plot_jitter,
        # x.var='SampleID',
        # y.var='log10.qval',
        # color.var='Genotype',
        # shape.var='Genotype',
        x.var='chr',
        y.var='log10.qval',
        color.var='enrichment',
        facet.row='SampleID',
        size=0.25,
        axis.title.x=element_blank(),
        axis.text.x=element_text(angle=45, hjust=1)
    )
```

# Volcano-ish plot Plots {.tabset}

## Genome-Wide {.tabset}

```{r volcanoish_gw, results='asis', fig.dim=c(10,8)}
loops.df %>% 
    # filter(weight == 'raw', resolution == '10Kb') %>% 
    make_nested_plot_tabs(
        # group.cols=c('type', 'isMerged', 'weight', 'resolution'),
        # group.cols=c('weight', 'resolution', 'metric', 'kernel'),
        group.cols=c('weight', 'resolution', 'kernel'),
        max.header.lvl=3,
        plot.fnc=plot_jitter,
        x.var='enrichment',
        y.var='log10.qval',
        color.var='SampleID',
        regression_fnc='lm',
        add_regression_SE=TRUE,
        # facet.group='chr',
        # facet.nrow=4,
        scales='fixed',
        legend.position='right',
        # size=0.25,
        axis.text.x=element_text(angle=45, hjust=1)
    )
```

## Per Chr {.tabset}

```{r volcanoish_chr, results='asis', fig.dim=c(10,6)}
loops.df %>% 
    # filter(weight == 'raw', resolution == '10Kb') %>% 
    make_nested_plot_tabs(
        # group.cols=c('type', 'isMerged', 'weight', 'resolution'),
        # group.cols=c('weight', 'resolution', 'metric', 'kernel'),
        group.cols=c('weight', 'resolution', 'kernel'),
        max.header.lvl=3,
        plot.fnc=plot_jitter,
        x.var='enrichment',
        y.var='log10.qval',
        color.var='SampleID',
        regression_fnc='lm',
        add_regression_SE=TRUE,
        facet.group='chr',
        facet.nrow=4,
        scales='fixed',
        # legend.position='top',
        # size=0.25,
        strip.text=element_text(size=8),
        axis.text.x=element_text(angle=45, hjust=1)
    )
```

# Save Filtered Loop Results

```{r save_filtered_results}
loops.df %>% 
    filter_loop_results() %>% 
    write_tsv(FILTERED_COOLTOOLS_LOOPS_RESULTS_FILE)
```
